---
title: "class09_mini_project"
author: 'Hope (PID: A15652616)'
date: "10/26/2021"
output: html_document
---

Import data
```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
```

We don't want to include the analysis given in the file
```{r}
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]
```


setup a separate new vector called diagnosis that contains the data from the diagnosis column of the original dataset. We will store this as a factor (useful for plotting) and use this later to check our results.
```{r}
# Create diagnosis vector for later
diagnosis <- as.factor(c(wisc.df[,1]) )
```


#Q1. How many observations are in this dataset?
```{r}
dim(wisc.data)
```
There are 569 patients with 30 different measurements

#Q2. How many of the observations have a malignant diagnosis?
```{r}
table(diagnosis)
```
There were 212 people with a malignant diagnosis.

#Q3. How many variables/features in the data are suffixed with _mean?
```{r}
?grep
colnames(wisc.df)
length(grep("mean", colnames(wisc.df)))
```
There are 10 variables/features with the suffix "mean"


The next step in your analysis is to perform principal component analysis (PCA) on wisc.data.

Check the mean and standard deviation of the features (i.e. columns) of the wisc.data to determine if the data should be scaled. Use the colMeans() and apply() functions like youâ€™ve done before.
```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```


Execute PCA with the prcomp() function on the wisc.data, scaling if appropriate, and assign the output model to wisc.pr.
```{r}
# Perform PCA on wisc.data by completing the following code
wisc.pr <- prcomp(wisc.data, scale. = TRUE)
```

Summarize
```{r}
# Look at summary of results
summary(wisc.pr)
```

# Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
There is 44% of the original variance captured by PC1.

# Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
You need 3 principal components to describe at least 70% of the original variance in the data.

# Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
You need 7 principal components to describe at least 90% of the original variance in the data.

Look at biplot
```{r}
biplot(wisc.pr)
```
#Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
This plot is difficult to understand and everything is too close to be able to actually read.

We can separate this out a little better. 


Let's make a score plot
```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[, 1:2 ], col = as.factor(diagnosis))
```


#Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
```{r}
plot(wisc.pr$x[, c(1,3) ], col = as.factor(diagnosis), 
     xlab = "PC1", ylab = "PC3")
```
These graphs make it much easier to compare different principal components compared to the biplot seen before. We can see that PC1 explains the most variance compared to both PC2 and PC3. Overall we see there is a bunching of malignant vs benign samples.


We can also graph this using ggplot to make it look a little better.

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=as.factor(diagnosis)) + 
  geom_point()
```


We now want to explain the variance 

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Calculate the variance explained by each principal component by dividing by the total variance explained of all principal components. Assign this to a variable called pve and create a plot of variance explained for each principal component.
```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```


Look at a scree plot
```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

#Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?
```{r}
wisc.pr$rotation["concave.points_mean",1]
```


#Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?
```{r}
summary(wisc.pr)
```
You need 5 PC's to explain 80% of the variance 


##Let's start looking at hierarchical clustering







